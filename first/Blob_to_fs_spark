Blob to ftp - Snowpark
======================

--1.Create egress/outbound network rule
CREATE OR REPLACE NETWORK RULE my_ftp_network_rule
  TYPE = 'HOST_PORT'
  VALUE_LIST = ('<ftp_server_host>:<ftp_server_port>')
  MODE = 'EGRESS';

--2.Create ftp secret
 CREATE OR REPLACE SECRET my_ftp_secret
  TYPE = GENERIC_PASSWORD
  USERNAME = 'ftp_user'
  PASSWORD = '<ftp_password>';

--3.Create external access integration
  CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION my_ftp_integration
  ALLOWED_NETWORK_RULES = (my_ftp_network_rule)
  ALLOWED_AUTHENTICATION_SECRETS = (my_ftp_secret)
  ENABLED = TRUE;

--4.Copy azure to ftp
  CREATE OR REPLACE PROCEDURE copy_azure_to_ftp(
    azure_container STRING,
    azure_storage_account STRING,
    azure_sas_token STRING,
    ftp_remote_path STRING,
    ftp_secret_name STRING
)
RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python', 'azure-storage-blob', 'ftplib')
EXTERNAL_ACCESS_INTEGRATIONS = (my_ftp_integration)
SECRETS = ('ftp_cred' = my_ftp_secret)
HANDLER = 'run'
AS
$$
import ftplib
import snowflake.snowpark as snowpark
from azure.storage.blob import BlobServiceClient

def run(session, azure_container, azure_storage_account, azure_sas_token, ftp_remote_path, ftp_secret_name):
    # Retrieve FTP credentials from the secret
    ftp_credentials = session.get_secret(ftp_secret_name)
    ftp_host = ftp_credentials.get_string('host')
    ftp_user = ftp_credentials.get_string('username')
    ftp_pass = ftp_credentials.get_string('password')

    # Connect to Azure Blob Storage
    connect_str = f"BlobEndpoint=https://{azure_storage_account}.blob.core.windows.net/;SharedAccessSignature={azure_sas_token}"
    blob_service_client = BlobServiceClient.from_connection_string(connect_str)
    container_client = blob_service_client.get_container_client(azure_container)

    file_transfer_log = []

    try:
        with ftplib.FTP(ftp_host) as ftp:
            ftp.login(user=ftp_user, passwd=ftp_pass)

            # List files in Azure and transfer each one
            for blob in container_client.list_blobs():
                file_name = blob.name
                
                # Download file from Azure to a temporary local path in Snowflake's sandbox
                temp_file_path = f"/tmp/{file_name}"
                with open(temp_file_path, "wb") as download_file:
                    download_stream = container_client.get_blob_client(file_name).download_blob()
                    download_stream.readinto(download_file)
                
                # Upload file to the remote FTP server
                with open(temp_file_path, "rb") as upload_file:
                    ftp.storbinary(f'STOR {ftp_remote_path}/{file_name}', upload_file)
                
                file_transfer_log.append(f"Successfully transferred {file_name}")

    except Exception as e:
        return f"File transfer failed: {e}"

    return "\n".join(file_transfer_log)
$$;

CALL copy_azure_to_ftp(
    azure_container => 'my_data_container',
    azure_storage_account => 'my_storage_account',
    azure_sas_token => '<your_sas_token>',
    ftp_remote_path => '/destination_folder',
    ftp_secret_name => 'my_ftp_secret'
);



Azure credentials-
-----------------

CREATE OR REPLACE STAGE my_azure_stage
  URL = 'azure://<your_storage_account>.blob.core.windows.net/<your_container>/<optional_path>/'
  CREDENTIALS = (AZURE_SAS_TOKEN = '<your_copied_sas_token>'); -- new token gen post expiry

e.g.

USE ROLE DBA;
CREATE OR REPLACE STAGE VANTAGEDW.STAGING.STG_AZ_LANDING
URL = 'azure://fenixsaadls.blob.core.windows.net/input/'
CREDENTIALS = (AZURE_SAS_TOKEN = 'sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2026-01-10T12:57:49Z&st=2025-08-10T04:42:49Z&spr=https,http&sig=VxJJScjUWzRsKvMM%2FNXGr9lFELXbH3UHZJO4G3fUL7w%3D')
;


Azure tenant_id available via snowflake secrets / environment variable in customer vm
Azure client_id available via snowflake secrets / environment variable in customer vm
Azure client_secret available via snowflake secrets / environment variable in customer vm
Storage blob contributor role FOR THE APPLICATION created during storage integration
This is required to assign RBAC permissions the app user for snowflake service.

Azure key vault secret name for SAS_TOKEN - this will used as part of connection string to connect to 
blob storage with python.
Azure key_vault_secret_user RBAC permissions

Access Az Key vault-
-------------------

# Using Service principal - recommended in snowpark containers
# 3 secrets to be retrieved from snowflake secrets objects

import os
from azure.identity import ClientSecretCredential
from azure.keyvault.secrets import SecretClient
import sys

def get_secret_from_keyvault_in_container(keyvault_url: str, secret_name: str) -> str:
    """
    Accesses Azure Key Vault using a Service Principal (client ID, client secret, tenant ID).
    These credentials must be securely provided to your Snowpark Container (e.g., via Snowflake Secrets
    loaded as environment variables, or other secure injection methods).
    """
    try:
        # Retrieve credentials from environment variables (recommended for containers)
        # These environment variables would be securely passed to your Snowpark Container
        # For example, by referencing Snowflake Secrets in your service definition.
        tenant_id = os.environ.get("AZURE_TENANT_ID")
        client_id = os.environ.get("AZURE_CLIENT_ID")
        client_secret = os.environ.get("AZURE_CLIENT_SECRET")

        if not all([tenant_id, client_id, client_secret]):
            raise ValueError("Azure credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) not found in environment variables.")

        credential = ClientSecretCredential(
            tenant_id=tenant_id,
            client_id=client_id,
            client_secret=client_secret
        )
        
        secret_client = SecretClient(vault_url=keyvault_url, credential=credential)
        secret = secret_client.get_secret(secret_name)
        return secret.value

    except Exception as e:
        print(f"Error accessing Key Vault from Snowpark Container: {e}", file=sys.stderr)
        # Provide guidance for container setup
        print("Ensure the following environment variables are securely set for your container:", file=sys.stderr)
        print("- AZURE_TENANT_ID", file=sys.stderr)
        print("- AZURE_CLIENT_ID", file=sys.stderr)
        print("- AZURE_CLIENT_SECRET", file=sys.stderr)
        print("Also, ensure the Service Principal has 'Get' secret permission on the Key Vault.", file=sys.stderr)
        raise


# Example Usage (replace with your actual values and container setup)
if __name__ == "__main__":
    # In your Snowpark Container service definition, you would typically
    # map Snowflake Secrets to these environment variables.
    # Example:
    # env:
    #   AZURE_TENANT_ID: SNOWFLAKE_SECRET_TO_AZURE_TENANT_ID
    #   AZURE_CLIENT_ID: SNOWFLAKE_SECRET_TO_AZURE_CLIENT_ID
    #   AZURE_CLIENT_SECRET: SNOWFLAKE_SECRET_TO_AZURE_CLIENT_SECRET
    
    # These are placeholders; actual values come from container's environment
    keyvault_url = "https://your-keyvault-name.vault.azure.net/"
    secret_name = "MyContainerSecret"

    try:
        retrieved_secret = get_secret_from_keyvault_in_container(keyvault_url, secret_name)
        print(f"Successfully retrieved secret: {retrieved_secret}")
    except Exception as e:
        print(f"Failed to retrieve secret: {e}")


Create and retrieve snowflake secrets-
-------------------------------------

-- Create a secret for the Azure tenant ID
CREATE OR REPLACE SECRET azure_tenant_secret
  TYPE = GENERIC_STRING
  SECRET_STRING = '<your_azure_tenant_id>';

-- Create a secret for the Azure client ID
CREATE OR REPLACE SECRET azure_client_id_secret
  TYPE = GENERIC_STRING
  SECRET_STRING = '<your_azure_client_id>';

-- Create a secret for the Azure client secret
CREATE OR REPLACE SECRET azure_client_secret
  TYPE = GENERIC_STRING
  SECRET_STRING = '<your_azure_client_secret>';


CREATE OR REPLACE PROCEDURE my_azure_proc()
  RETURNS STRING
  LANGUAGE PYTHON
  RUNTIME_VERSION = '3.8'
  PACKAGES = ('_snowflake')
  SECRETS = (
    'azure_tenant' = azure_tenant_secret,
    'azure_client_id' = azure_client_id_secret,
    'azure_client_secret' = azure_client_secret
  )
  HANDLER = 'run'
AS
$$
import _snowflake

def run(session):
  # Retrieve the secret values using the aliases defined in the SECRETS parameter
  azure_tenant_id = _snowflake.get_generic_secret_string('azure_tenant')
  client_id = _snowflake.get_generic_secret_string('azure_client_id')
  client_secret = _snowflake.get_generic_secret_string('azure_client_secret')
  
  # Now you can use these variables to authenticate with Azure
  return f"Retrieved credentials: Tenant ID={azure_tenant_id}, Client ID={client_id}"
$$;

--retrieve json string secrets from az key vault
===============================================

from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient
import json

# Authenticate and connect
vault_url = "https://<your-key-vault-name>.vault.azure.net/"
credential = DefaultAzureCredential()
client = SecretClient(vault_url=vault_url, credential=credential)

# Get secret (string)
secret = client.get_secret("db-credentials").value

# Convert JSON string back to dict
credentials = json.loads(secret)

print(credentials["username"])  # john_doe
print(credentials["password"])  # secret123

azure sql database to snowflake using snowpark python-
=====================================================
Limitation - Cannot be used in trial accounts

CREATE OR REPLACE NETWORK RULE azure_sql_network_rule
  TYPE = 'HOST_PORT'
  VALUE_LIST = ('externalaccessdemo.database.windows.net:1433')
  MODE = 'EGRESS';

CREATE OR REPLACE SECRET azure_sql_secret
  TYPE = GENERIC_PASSWORD
  USERNAME = '<your_sql_username>'
  PASSWORD = '<your_sql_password>';

CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION azure_sql_integration
  ALLOWED_NETWORK_RULES = (azure_sql_network_rule)
  ALLOWED_AUTHENTICATION_SECRETS = (azure_sql_secret)
  ENABLED = TRUE;

CREATE OR REPLACE PROCEDURE pull_azure_data()
RETURNS TABLE()
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python', 'pandas', 'pyodbc')
EXTERNAL_ACCESS_INTEGRATIONS = (azure_sql_integration)
SECRETS = ('azure_sql_secret' = azure_sql_secret)
HANDLER = 'run'
AS
import pandas as pd
from sqlalchemy import create_engine
from snowflake.connector.pandas_tools import write_pandas

  """
    Connects to Azure SQL Server using External Access Integration,
    pulls data from a specified table, and returns a Snowpark DataFrame.
    """
    # 1. Retrieve connection details from Snowflake Secrets
    # It's assumed a secret named 'azure_sql_secret' has been created in Snowflake
    # with the keys: server, database, username, and password.
    secret_name = "azure_sql_secret"
    secret = session.get_secret(secret_name)
    
    server = secret.get_string("server")
    database = secret.get_string("database")
    username = secret.get_string("username")
    password = secret.get_string("password")

    # 2. Establish connection to Azure SQL Server
    source_conn_str = (
        f"DRIVER={{ODBC Driver 17 for SQL Server}};"
        f"SERVER={server};"
        f"DATABASE={database};"
        f"UID={username};"
        f"PWD={password};"
        "Encrypt=yes;"
        "TrustServerCertificate=no;"
        "Connection Timeout=30;"
    )

    chunksize = 10000
    source_query = f"SELECT * FROM {table_name};"
    target_table_name = 'DBO.EMPLOYEE'


def read_and_write_in_chunks(source_conn_str, source_query, chunksize, snowflake_session, target_table_name):
    """
    Reads data from a source database in chunks, loads it into Pandas DataFrames,
    and then writes each chunk to Snowflake.

    Args:
        source_conn_str (str): The SQLAlchemy connection string for the source database.
        source_query (str): The SQL query to execute on the source database.
        chunksize (int): The number of rows to read in each chunk.
        snowflake_session (snowflake.snowpark.Session): The active Snowpark session.
        target_table_name (str): The name of the Snowflake table to write to.
    """
    try:
        # Create a SQLAlchemy engine for the source database
        engine = create_engine(source_conn_str)
        
        # Use pandas read_sql with chunksize to iterate over the data
        with engine.connect() as connection:
            for chunk_df in pd.read_sql(source_query, connection, chunksize=chunksize):
                print(f"Read a chunk of {len(chunk_df)} rows from the source database.")
                
                # Write each pandas DataFrame chunk to Snowflake
                success, n_chunks, n_rows, _ = write_pandas(
                    conn=snowflake_session.connection,
                    df=chunk_df,
                    table_name=target_table_name,
                    auto_create_table=False,
                    overwrite=False,
                    database=snowflake_session.get_current_database(),
                    schema=snowflake_session.get_current_schema(),
                )
                
                if success:
                    print(f"Successfully wrote {n_rows} rows to Snowflake table '{target_table_name}'.")
                else:
                    print(f"Failed to write chunk to Snowflake.")
                    break
        
    except Exception as e:
        print(f"An error occurred: {e}")



--update config replacing env variables #ev_*# in file
======================================================
import configparser
import os
import re

# Custom exception for missing environment variables
class MissingEnvironmentVariableError(Exception):
    def __init__(self, missing_vars):
        self.missing_vars = missing_vars
        super().__init__(f"The following environment variables are not configured: {', '.join(missing_vars)}")

def replace_tokens_from_config(file_content, config_file_path):
    """
    Replaces tokens in a string with values from environment variables specified in a config file.

    Args:
        file_content (str): The string content with tokens.
        config_file_path (str): The path to the config.ini file.

    Returns:
        str: The modified content.
    """
    config = configparser.ConfigParser()
    config.read(config_file_path)

    # Dictionary to store the token-to-env-var mapping
    token_map = {}
    if 'variables' in config:
        token_map = dict(config['variables'])
    else:
        raise ValueError("Config file must have a '[variables]' section.")

    # Find all tokens in the file content
    token_pattern = re.compile(r'#ev_(\w+)#')
    tokens_found = set(token_pattern.findall(file_content))
    
    missing_vars = []
    replacements = {}

    # Check for and retrieve the corresponding environment variables
    for token in tokens_found:
        config_key = f'ev_{token}'
        if config_key not in token_map:
            raise ValueError(f"Token '{token}' found in file but not in config file.")

        env_var_name = token_map[config_key]
        env_value = os.getenv(env_var_name)
        
        if env_value is None:
            missing_vars.append(env_var_name)
        else:
            replacements[f'#ev_{token}#'] = env_value

    if missing_vars:
        raise MissingEnvironmentVariableError(missing_vars)

    # Perform the replacement using a regex substitution function
    def replace_match(match):
        token = match.group(0)
        return replacements.get(token, token)
        
    final_content = token_pattern.sub(replace_match, file_content)

    return final_content

if __name__ == "__main__":
    # Mock environment variables for demonstration
    os.environ['LANDINGMINOR'] = 'landing_db_dev'
    os.environ['STAGINGMINOR'] = 'staging_db_dev'
    os.environ['DEV'] = 'dev_env'
    os.environ['EDR_DEV'] = 'edr_db_dev'
    
    # Example file content
    file_contents = """
    select 
    *
    from #ev_landing_db#.landing_schema.claims
    join #ev_staging_db#.staging_schema.stg_Claims
    where 
        '#ev_env_name#' = 'dev'

    select 
    *
    from #ev_edr_db#.landing_schema.claims
    join #ev_staging_db#.staging_schema.stg_Claims
    where 
        '#ev_env_name#' = 'dev'
    """
    
    # Create a mock config.ini file for the script to read
    with open('config.ini', 'w') as f:
        f.write("[variables]\n")
        f.write("ev_landing_db=LANDINGMINOR\n")
        f.write("ev_staging_db=STAGINGMINOR\n")
        f.write("ev_env_name=DEV\n")
        f.write("ev_edr_db=EDR_DEV\n")

    try:
        # Example of a successful replacement
        print("--- Successfully running with all variables configured ---")
        modified_sql = replace_tokens_from_config(file_contents, 'config.ini')
        print(modified_sql)

        # Example of a failed run due to a missing environment variable
        print("\n--- Running with a missing environment variable ---")
        os.environ.pop('DEV', None) # Remove a variable to force an error
        replace_tokens_from_config(file_contents, 'config.ini')

    except (ValueError, MissingEnvironmentVariableError) as e:
        print(f"Error: {e}")



-- DPC DBT core run artifacts
=============================

-- file:///<dbt_run_folder_path>
--azure://<container_name>/bcbsri-datahub/dbt_artifacts/target_123

Set Environment Variables: In the dbt Core component, use the set_env_var property to define the location where dbt will output its target folder. This path will be used by the Python script.
{
  "set_env_var": {
    "DBT_TARGET_DIR": "${MATILLION_AGENT_WORK_DIR}/dbt-run-${matillion_pipeline_id}/target"
  }
}

# Import required libraries
from azure.storage.blob import BlobServiceClient
import os
import snowflake.snowpark as snowpark

# Define the script handler function
def run(session):
    """
    Moves dbt artifacts from the local temporary directory to Azure Blob Storage.
    
    Args:
        session (snowflake.snowpark.Session): The Snowpark session object.
    
    Returns:
        str: A message indicating the outcome of the transfer.
    """
    # 1. Retrieve Azure credentials securely from a Snowflake Secret
    azure_credentials_secret_name = 'your_azure_credentials_secret'
    # The secret should be of type GENERIC_PASSWORD with keys 'account_name' and 'sas_token'
    azure_credentials = session.get_secret(azure_credentials_secret_name)
    azure_account_name = azure_credentials.get_string('account_name')
    azure_sas_token = azure_credentials.get_string('sas_token')
    
    # 2. Define source and destination paths
    dbt_target_dir = os.getenv('DBT_TARGET_DIR', '/tmp/dbt/target')
    azure_container = 'dbt-artifacts-container'
    
    # Generate a unique path for the run in Azure Blob Storage
    pipeline_run_id = os.getenv('MATILLION_JOB_ID')
    destination_path = f'artifacts/{pipeline_run_id}'
    
    if not os.path.exists(dbt_target_dir):
        return f"Warning: dbt target directory not found at {dbt_target_dir}. No artifacts to move."
        
    try:
        # 3. Connect to Azure Blob Storage
        connect_str = f"DefaultEndpointsProtocol=https;AccountName={azure_account_name};SharedAccessSignature={azure_sas_token}"
        blob_service_client = BlobServiceClient.from_connection_string(connect_str)
        container_client = blob_service_client.get_container_client(azure_container)
        
        # 4. Iterate through files and upload
        upload_count = 0
        for root, _, files in os.walk(dbt_target_dir):
            for file_name in files:
                local_file_path = os.path.join(root, file_name)
                relative_path = os.path.relpath(local_file_path, dbt_target_dir)
                blob_name = f"{destination_path}/{relative_path}"
                
                with open(local_file_path, "rb") as data:
                    container_client.upload_blob(name=blob_name, data=data, overwrite=True)
                    upload_count += 1
                    
        return f"Successfully uploaded {upload_count} artifacts to Azure Blob Storage."
        
    except Exception as e:
        # 5. Handle any errors during the upload process
        return f"Error during artifact upload: {str(e)}"     