{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**python chunking method is faster than pandas** <br>\n",
    "On a 3 gb file with 11.8 million rows of data ,comparatively it takes 9 seconds with python and 160 secs with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=40000\n",
    "\n",
    "def write_chunk(part_id , lines):\n",
    "     with open(\"C:\\\\Users\\\\conne\\\\snowflake\\\\files\\\\load\\\\s3_data\\\\1995_part\"+str(part_id)+\".csv\" , 'w') as fw:\n",
    "          fw.write(header);\n",
    "          fw.writelines(lines);\n",
    "     \n",
    "\n",
    "with open(\"C:\\\\Users\\\\conne\\\\snowflake\\\\files\\\\load\\\\s3_data\\\\1995\\\\1995_data.csv\", \"r\") as f:\n",
    "    count = 0\n",
    "    lines = []\n",
    "    header = f.readline() #this will repeat in  all files\n",
    "    for l in f:\n",
    "        lines.append(l)\n",
    "        count += 1 #loop first 40000 lines and accumulate the counter till 40000 to be divisible by it\n",
    "        if count % batch_size == 0 : #first 40k, next 80k and so on\n",
    "            write_chunk(count // batch_size , lines )   # part file number (floor division) and lines list\n",
    "            lines = [] #reset in order to hold next set of lines\n",
    "    \n",
    "    if len(lines) > 0:\n",
    "          # we need this as the count won't be divisible by the batch size\n",
    "          write_chunk((count // batch_size) + 1, lines ) #the remaining part ,say 26000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas to enumerate thru chunks of data and loading into small files or even to a table\n",
    "\n",
    "import pandas as pd\n",
    "#from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "source_path = \"C:\\\\Users\\\\conne\\\\snowflake\\\\files\\\\load\\\\s3_data\\\\1995\\\\1995_data.csv\"\n",
    "for i,chunk in enumerate(pd.read_csv(source_path, chunksize=40000)):\n",
    "    chunk.to_csv('C:\\\\Users\\\\conne\\\\snowflake\\\\files\\\\load\\\\s3_data\\\\chunk{}.csv'.format(i), index=False)\n",
    "\n",
    "# for i,chunk in enumerate(pd.read_csv(source_path, chunksize=40000)):\n",
    "#     write_pandas(conn,df,table,database,schema)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "source_path = \"C:\\\\Users\\\\conne\\\\snowflake\\\\files\\\\load\\\\s3_data\\\\1995\\\\1995_data.csv\"\n",
    "\n",
    "for idx,chunk in enumerate(pd.read_csv(source_path , chunksize = 40000)):\n",
    "    chunk.to_csv(f'C:\\\\Users\\\\conne\\\\snowflake\\\\files\\\\load\\\\s3_data\\\\part{idx}.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BytesIO object at 0x000002AC965166D0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "source_path = \"C:\\\\Users\\\\conne\\\\snowflake\\\\files\\\\load\\\\s3_data\\\\1995\\\\1995_data.csv\"\n",
    "buf = BytesIO(open(source_path,\"rb\").read(8192));\n",
    "zipped = zipfile.ZipFile(buf)\n",
    "zipped.namelist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysparkvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
